# =============================================================================
# Development Config - E2E Model (RTX 2050 4GB)
# =============================================================================
# Minimal config using wav2vec2-base for low-VRAM testing

# Model
model:
  type: "e2e"
  encoder: "facebook/wav2vec2-base" # Smaller encoder
  decoder: "facebook/mbart-large-50"
  add_adapter: true
  freeze_encoder: true # Critical for 4GB VRAM

# Training - Very conservative for large model on small GPU
training:
  batch_size: 1
  gradient_accumulation_steps: 8
  # Effective batch size: 8

  learning_rate: 1.0e-5
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.15
  weight_decay: 0.01
  max_grad_norm: 1.0

  num_train_epochs: 2
  max_steps: 100 # Quick test

  fp16: true
  gradient_checkpointing: true # Essential

  eval_steps: 25
  save_steps: 50
  logging_steps: 10

  # Encoder freezing strategy
  freeze_feature_encoder: true
  freeze_encoder_epochs: 999 # Keep frozen for dev

# DataLoader
dataloader:
  num_workers: 2
  pin_memory: true

# Output
output:
  dir: "training/outputs/dev_e2e"
  experiment_name: "e2e_base_dev"
