<!DOCTYPE html><html><head>
      <title>08_preprocessing_raw_problems</title>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      
      <link rel="stylesheet" href="file:///c:\Users\Quang Chien\.vscode\extensions\shd101wyy.markdown-preview-enhanced-0.8.20\crossnote\dependencies\katex\katex.min.css">
      
      
      
      
      
      <style>
      code[class*=language-],pre[class*=language-]{color:#333;background:0 0;font-family:Consolas,"Liberation Mono",Menlo,Courier,monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.4;-moz-tab-size:8;-o-tab-size:8;tab-size:8;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none}pre[class*=language-]{padding:.8em;overflow:auto;border-radius:3px;background:#f5f5f5}:not(pre)>code[class*=language-]{padding:.1em;border-radius:.3em;white-space:normal;background:#f5f5f5}.token.blockquote,.token.comment{color:#969896}.token.cdata{color:#183691}.token.doctype,.token.macro.property,.token.punctuation,.token.variable{color:#333}.token.builtin,.token.important,.token.keyword,.token.operator,.token.rule{color:#a71d5d}.token.attr-value,.token.regex,.token.string,.token.url{color:#183691}.token.atrule,.token.boolean,.token.code,.token.command,.token.constant,.token.entity,.token.number,.token.property,.token.symbol{color:#0086b3}.token.prolog,.token.selector,.token.tag{color:#63a35c}.token.attr-name,.token.class,.token.class-name,.token.function,.token.id,.token.namespace,.token.pseudo-class,.token.pseudo-element,.token.url-reference .token.variable{color:#795da3}.token.entity{cursor:help}.token.title,.token.title .token.punctuation{font-weight:700;color:#1d3e81}.token.list{color:#ed6a43}.token.inserted{background-color:#eaffea;color:#55a532}.token.deleted{background-color:#ffecec;color:#bd2c00}.token.bold{font-weight:700}.token.italic{font-style:italic}.language-json .token.property{color:#183691}.language-markup .token.tag .token.punctuation{color:#333}.language-css .token.function,code.language-css{color:#0086b3}.language-yaml .token.atrule{color:#63a35c}code.language-yaml{color:#183691}.language-ruby .token.function{color:#333}.language-markdown .token.url{color:#795da3}.language-makefile .token.symbol{color:#795da3}.language-makefile .token.variable{color:#183691}.language-makefile .token.builtin{color:#0086b3}.language-bash .token.keyword{color:#0086b3}pre[data-line]{position:relative;padding:1em 0 1em 3em}pre[data-line] .line-highlight-wrapper{position:absolute;top:0;left:0;background-color:transparent;display:block;width:100%}pre[data-line] .line-highlight{position:absolute;left:0;right:0;padding:inherit 0;margin-top:1em;background:hsla(24,20%,50%,.08);background:linear-gradient(to right,hsla(24,20%,50%,.1) 70%,hsla(24,20%,50%,0));pointer-events:none;line-height:inherit;white-space:pre}pre[data-line] .line-highlight:before,pre[data-line] .line-highlight[data-end]:after{content:attr(data-start);position:absolute;top:.4em;left:.6em;min-width:1em;padding:0 .5em;background-color:hsla(24,20%,50%,.4);color:#f4f1ef;font:bold 65%/1.5 sans-serif;text-align:center;vertical-align:.3em;border-radius:999px;text-shadow:none;box-shadow:0 1px #fff}pre[data-line] .line-highlight[data-end]:after{content:attr(data-end);top:auto;bottom:.4em}html body{font-family:'Helvetica Neue',Helvetica,'Segoe UI',Arial,freesans,sans-serif;font-size:16px;line-height:1.6;color:#333;background-color:#fff;overflow:initial;box-sizing:border-box;word-wrap:break-word}html body>:first-child{margin-top:0}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{line-height:1.2;margin-top:1em;margin-bottom:16px;color:#000}html body h1{font-size:2.25em;font-weight:300;padding-bottom:.3em}html body h2{font-size:1.75em;font-weight:400;padding-bottom:.3em}html body h3{font-size:1.5em;font-weight:500}html body h4{font-size:1.25em;font-weight:600}html body h5{font-size:1.1em;font-weight:600}html body h6{font-size:1em;font-weight:600}html body h1,html body h2,html body h3,html body h4,html body h5{font-weight:600}html body h5{font-size:1em}html body h6{color:#5c5c5c}html body strong{color:#000}html body del{color:#5c5c5c}html body a:not([href]){color:inherit;text-decoration:none}html body a{color:#08c;text-decoration:none}html body a:hover{color:#00a3f5;text-decoration:none}html body img{max-width:100%}html body>p{margin-top:0;margin-bottom:16px;word-wrap:break-word}html body>ol,html body>ul{margin-bottom:16px}html body ol,html body ul{padding-left:2em}html body ol.no-list,html body ul.no-list{padding:0;list-style-type:none}html body ol ol,html body ol ul,html body ul ol,html body ul ul{margin-top:0;margin-bottom:0}html body li{margin-bottom:0}html body li.task-list-item{list-style:none}html body li>p{margin-top:0;margin-bottom:0}html body .task-list-item-checkbox{margin:0 .2em .25em -1.8em;vertical-align:middle}html body .task-list-item-checkbox:hover{cursor:pointer}html body blockquote{margin:16px 0;font-size:inherit;padding:0 15px;color:#5c5c5c;background-color:#f0f0f0;border-left:4px solid #d6d6d6}html body blockquote>:first-child{margin-top:0}html body blockquote>:last-child{margin-bottom:0}html body hr{height:4px;margin:32px 0;background-color:#d6d6d6;border:0 none}html body table{margin:10px 0 15px 0;border-collapse:collapse;border-spacing:0;display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}html body table th{font-weight:700;color:#000}html body table td,html body table th{border:1px solid #d6d6d6;padding:6px 13px}html body dl{padding:0}html body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:700}html body dl dd{padding:0 16px;margin-bottom:16px}html body code{font-family:Menlo,Monaco,Consolas,'Courier New',monospace;font-size:.85em;color:#000;background-color:#f0f0f0;border-radius:3px;padding:.2em 0}html body code::after,html body code::before{letter-spacing:-.2em;content:'\00a0'}html body pre>code{padding:0;margin:0;word-break:normal;white-space:pre;background:0 0;border:0}html body .highlight{margin-bottom:16px}html body .highlight pre,html body pre{padding:1em;overflow:auto;line-height:1.45;border:#d6d6d6;border-radius:3px}html body .highlight pre{margin-bottom:0;word-break:normal}html body pre code,html body pre tt{display:inline;max-width:initial;padding:0;margin:0;overflow:initial;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}html body pre code:after,html body pre code:before,html body pre tt:after,html body pre tt:before{content:normal}html body blockquote,html body dl,html body ol,html body p,html body pre,html body ul{margin-top:0;margin-bottom:16px}html body kbd{color:#000;border:1px solid #d6d6d6;border-bottom:2px solid #c7c7c7;padding:2px 4px;background-color:#f0f0f0;border-radius:3px}@media print{html body{background-color:#fff}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{color:#000;page-break-after:avoid}html body blockquote{color:#5c5c5c}html body pre{page-break-inside:avoid}html body table{display:table}html body img{display:block;max-width:100%;max-height:100%}html body code,html body pre{word-wrap:break-word;white-space:pre}}.markdown-preview{width:100%;height:100%;box-sizing:border-box}.markdown-preview ul{list-style:disc}.markdown-preview ul ul{list-style:circle}.markdown-preview ul ul ul{list-style:square}.markdown-preview ol{list-style:decimal}.markdown-preview ol ol,.markdown-preview ul ol{list-style-type:lower-roman}.markdown-preview ol ol ol,.markdown-preview ol ul ol,.markdown-preview ul ol ol,.markdown-preview ul ul ol{list-style-type:lower-alpha}.markdown-preview .newpage,.markdown-preview .pagebreak{page-break-before:always}.markdown-preview pre.line-numbers{position:relative;padding-left:3.8em;counter-reset:linenumber}.markdown-preview pre.line-numbers>code{position:relative}.markdown-preview pre.line-numbers .line-numbers-rows{position:absolute;pointer-events:none;top:1em;font-size:100%;left:0;width:3em;letter-spacing:-1px;border-right:1px solid #999;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.markdown-preview pre.line-numbers .line-numbers-rows>span{pointer-events:none;display:block;counter-increment:linenumber}.markdown-preview pre.line-numbers .line-numbers-rows>span:before{content:counter(linenumber);color:#999;display:block;padding-right:.8em;text-align:right}.markdown-preview .mathjax-exps .MathJax_Display{text-align:center!important}.markdown-preview:not([data-for=preview]) .code-chunk .code-chunk-btn-group{display:none}.markdown-preview:not([data-for=preview]) .code-chunk .status{display:none}.markdown-preview:not([data-for=preview]) .code-chunk .output-div{margin-bottom:16px}.markdown-preview .md-toc{padding:0}.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link{display:inline;padding:.25rem 0}.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link div,.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link p{display:inline}.markdown-preview .md-toc .md-toc-link-wrapper.highlighted .md-toc-link{font-weight:800}.scrollbar-style::-webkit-scrollbar{width:8px}.scrollbar-style::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}.scrollbar-style::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,.66);border:4px solid rgba(150,150,150,.66);background-clip:content-box}html body[for=html-export]:not([data-presentation-mode]){position:relative;width:100%;height:100%;top:0;left:0;margin:0;padding:0;overflow:auto}html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{position:relative;top:0;min-height:100vh}@media screen and (min-width:914px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{padding:2em calc(50% - 457px + 2em)}}@media screen and (max-width:914px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{font-size:14px!important;padding:1em}}@media print{html body[for=html-export]:not([data-presentation-mode]) #sidebar-toc-btn{display:none}}html body[for=html-export]:not([data-presentation-mode]) #sidebar-toc-btn{position:fixed;bottom:8px;left:8px;font-size:28px;cursor:pointer;color:inherit;z-index:99;width:32px;text-align:center;opacity:.4}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] #sidebar-toc-btn{opacity:1}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc{position:fixed;top:0;left:0;width:300px;height:100%;padding:32px 0 48px 0;font-size:14px;box-shadow:0 0 4px rgba(150,150,150,.33);box-sizing:border-box;overflow:auto;background-color:inherit}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar{width:8px}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,.66);border:4px solid rgba(150,150,150,.66);background-clip:content-box}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc a{text-decoration:none}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc{padding:0 16px}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link{display:inline;padding:.25rem 0}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link div,html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link p{display:inline}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper.highlighted .md-toc-link{font-weight:800}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{left:300px;width:calc(100% - 300px);padding:2em calc(50% - 457px - 300px / 2);margin:0;box-sizing:border-box}@media screen and (max-width:1274px){html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{width:100%}}html body[for=html-export]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .markdown-preview{left:50%;transform:translateX(-50%)}html body[for=html-export]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .md-sidebar-toc{display:none}
/* Please visit the URL below for more information: */
/*   https://shd101wyy.github.io/markdown-preview-enhanced/#/customize-css */

      </style>
      <!-- The content below will be included at the end of the <head> element. --><script type="text/javascript">
  document.addEventListener("DOMContentLoaded", function () {
    // your code here
  });
</script></head><body for="html-export">
    
    
      <div class="crossnote markdown-preview  ">
      
<h1 id="08-preprocessing-raw-data-problems--solutions">08. Preprocessing Raw Data: Problems &amp; Solutions </h1>
<p>This document catalogs all known challenges in the preprocessing pipeline for Vietnamese-English Code-Switching (CS) Speech Translation data. Use this as a planning reference before implementation.</p>
<hr>
<h2 id="table-of-contents">Table of Contents </h2>
<ol>
<li><a href="#1-audio-segmentation-problems">Audio Segmentation Problems</a></li>
<li><a href="#2-signal-enhancement-problems">Signal Enhancement Problems</a></li>
<li><a href="#3-tts-synthesis-problems">TTS Synthesis Problems</a></li>
<li><a href="#4-cross-cutting-infrastructure-problems">Cross-Cutting Infrastructure Problems</a></li>
<li><a href="#5-data-quality--validation-problems">Data Quality &amp; Validation Problems</a></li>
<li><a href="#6-additional-workflow-problems">Additional Workflow Problems</a></li>
<li><a href="#7-decision-matrix">Decision Matrix</a></li>
<li><a href="#8-next-steps">Next Steps</a></li>
</ol>
<hr>
<h2 id="1-audio-segmentation-problems">1. Audio Segmentation Problems </h2>
<h3 id="11-mfa-dictionary-mismatch-code-switching">1.1 MFA Dictionary Mismatch (Code-Switching) </h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Problem</strong></td>
<td>Montreal Forced Aligner (MFA) uses a <strong>single pronunciation dictionary</strong> per run. Vietnamese-English CS audio contains words from both languages, but MFA's Vietnamese dictionary does not include English phonemes (and vice versa). Running with only the Vietnamese dictionary causes English words to fail alignment or be forced into incorrect Vietnamese phoneme sequences.</td>
</tr>
<tr>
<td><strong>Impact</strong></td>
<td>Misaligned timestamps, dropped words, garbage phoneme outputs for English tokens.</td>
</tr>
<tr>
<td><strong>Proposed Solutions</strong></td>
<td></td>
</tr>
<tr>
<td>Option A</td>
<td><strong>Merged Dictionary</strong>: Create a custom dictionary combining Vietnamese phonemes (from <code>vietnamese_mfa</code>) and English phonemes (from <code>english_us_arpa</code>). Requires manual phoneme mapping to avoid conflicts (e.g., Vietnamese <code>a</code> vs English <code>æ</code>).</td>
</tr>
<tr>
<td>Option B</td>
<td><strong>Two-Pass Alignment</strong>: Run MFA twice (once with Vietnamese model, once with English model), then merge alignments by selecting the higher-confidence result per word. Slower but avoids dictionary conflicts.</td>
</tr>
<tr>
<td>Option C</td>
<td><strong>Hybrid LID-First</strong>: Use word-level Language ID to tag each word before alignment, then route Vietnamese words to Vietnamese MFA and English words to English MFA in a single coordinated pass.</td>
</tr>
<tr>
<td><strong>Recommendation</strong></td>
<td>Start with <strong>Option B</strong> (two-pass) for simplicity. Migrate to <strong>Option A</strong> (merged dictionary) once phoneme mapping is validated.</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="12-timestamp-drift-between-youtube-captions-and-audio">1.2 Timestamp Drift Between YouTube Captions and Audio </h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Problem</strong></td>
<td>YouTube caption timestamps are approximate (often rounded to 0.1s or 0.5s boundaries). The actual speech may start 100-500ms before or after the caption timestamp. This drift accumulates over long videos.</td>
</tr>
<tr>
<td><strong>Impact</strong></td>
<td>MFA alignment fails when the seed timestamps are too far from reality. Segments may cut off word beginnings/endings.</td>
</tr>
<tr>
<td><strong>Proposed Solutions</strong></td>
<td></td>
</tr>
<tr>
<td>Option A</td>
<td><strong>Pre-alignment VAD Pass</strong>: Run Voice Activity Detection (VAD) first to identify actual speech regions, then snap YouTube timestamps to nearest VAD boundary before MFA.</td>
</tr>
<tr>
<td>Option B</td>
<td><strong>MFA Beam Search</strong>: Increase MFA's beam width to allow more flexibility in finding the correct alignment despite drift. Slower but more robust.</td>
</tr>
<tr>
<td>Option C</td>
<td><strong>Chunk-wise Alignment</strong>: Split audio into 30-60 second chunks with overlapping boundaries, align each chunk independently, then stitch results. Prevents drift accumulation.</td>
</tr>
<tr>
<td><strong>Recommendation</strong></td>
<td><strong>Option C</strong> (chunk-wise) combined with <strong>Option A</strong> (VAD pre-pass) for best results.</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="13-missing-transcripts-auto-generated-only-or-none">1.3 Missing Transcripts (Auto-Generated Only or None) </h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Problem</strong></td>
<td>Some YouTube videos have only auto-generated captions (lower quality) or no captions at all. Auto-generated captions often hallucinate words, especially for CS content where the ASR model is confused by language mixing.</td>
</tr>
<tr>
<td><strong>Impact</strong></td>
<td>Cannot use transcript-aware MFA alignment. Must fall back to transcript-free methods.</td>
</tr>
<tr>
<td><strong>Proposed Solutions</strong></td>
<td></td>
</tr>
<tr>
<td>Option A</td>
<td><strong>VAD-Only Segmentation</strong>: Use Silero VAD or pyannote.audio to detect speech segments without any transcript. Results in "blind" segments that need ASR downstream.</td>
</tr>
<tr>
<td>Option B</td>
<td><strong>ASR-First Pipeline</strong>: Run Whisper (large-v3) to generate a transcript first, then feed that transcript to MFA for refinement. Whisper handles CS better than YouTube's ASR.</td>
</tr>
<tr>
<td>Option C</td>
<td><strong>Skip and Flag</strong>: Mark these samples as <code>NEEDS_MANUAL_TRANSCRIPT</code> and defer to human annotation in Label Studio.</td>
</tr>
<tr>
<td><strong>Recommendation</strong></td>
<td><strong>Option B</strong> (Whisper → MFA) for videos with auto-captions. <strong>Option A</strong> (VAD-only) for videos with no captions at all.</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="14-speaker-diarization-in-multi-speaker-videos">1.4 Speaker Diarization in Multi-Speaker Videos </h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Problem</strong></td>
<td>Vlogs often feature multiple speakers (host + guest, interviews, reactions). MFA aligns the transcript to the audio but does not identify <em>who</em> is speaking. Segments may contain overlapping speech or speaker changes mid-segment.</td>
</tr>
<tr>
<td><strong>Impact</strong></td>
<td>Training data quality degrades if different speakers are mixed in a single sample. Speaker identity is lost.</td>
</tr>
<tr>
<td><strong>Proposed Solutions</strong></td>
<td></td>
</tr>
<tr>
<td>Option A</td>
<td><strong>pyannote.audio Diarization</strong>: Run speaker diarization before segmentation to identify speaker turns. Split segments at speaker boundaries, tag each segment with speaker ID.</td>
</tr>
<tr>
<td>Option B</td>
<td><strong>Single-Speaker Filter</strong>: Use diarization to estimate speaker count; if &gt;1 speaker, either skip the video or extract only the dominant speaker's segments.</td>
</tr>
<tr>
<td>Option C</td>
<td><strong>Ignore for Now</strong>: Accept mixed-speaker segments initially; refine later if needed for speaker-conditioned models.</td>
</tr>
<tr>
<td><strong>Recommendation</strong></td>
<td><strong>Option A</strong> (pyannote diarization) is ideal for quality. Start with <strong>Option C</strong> if resources are limited.</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="15-segment-length-variability">1.5 Segment Length Variability </h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Problem</strong></td>
<td>MFA outputs word-level alignments. Merging into sentence-level segments requires heuristics (punctuation, pause duration). This can produce segments ranging from 0.5s to 30s+ depending on the speaker's pacing and transcript punctuation quality.</td>
</tr>
<tr>
<td><strong>Impact</strong></td>
<td>Very short segments (&lt;1s) lack context. Very long segments (&gt;20s) are hard for downstream models and may span topic changes.</td>
</tr>
<tr>
<td><strong>Proposed Solutions</strong></td>
<td></td>
</tr>
<tr>
<td>Option A</td>
<td><strong>Fixed-Length Chunking</strong>: After MFA, re-segment into fixed 5-15 second chunks, splitting at word boundaries with minimum pause.</td>
</tr>
<tr>
<td>Option B</td>
<td><strong>Pause-Based Merging with Min/Max Bounds</strong>: Merge words into segments using pause threshold (e.g., 300ms), but enforce min=2s, max=15s constraints. Split long segments at sentence boundaries; merge short segments with neighbors.</td>
</tr>
<tr>
<td>Option C</td>
<td><strong>Hierarchical Segments</strong>: Keep both fine-grained (word-level) and coarse (sentence-level) alignments in database. Let downstream tasks choose granularity.</td>
</tr>
<tr>
<td><strong>Recommendation</strong></td>
<td><strong>Option B</strong> (bounded merging) for training data. Store <strong>Option C</strong> (hierarchical) in database for flexibility.</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="16-non-speech-audio-events">1.6 Non-Speech Audio Events </h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Problem</strong></td>
<td>Vlogs contain non-speech audio: music intros/outros, sound effects, laughter, coughs, filler sounds ("uhh", "hmm"). These are not in the transcript but occupy audio time.</td>
</tr>
<tr>
<td><strong>Impact</strong></td>
<td>MFA struggles to align silence/noise regions. Resulting segments may include unwanted audio at boundaries.</td>
</tr>
<tr>
<td><strong>Proposed Solutions</strong></td>
<td></td>
</tr>
<tr>
<td>Option A</td>
<td><strong>VAD Masking</strong>: Run VAD first, mask non-speech regions, only align within speech regions.</td>
</tr>
<tr>
<td>Option B</td>
<td><strong>Post-Alignment Trimming</strong>: After MFA, trim segment boundaries to exclude leading/trailing silence using energy-based detection.</td>
</tr>
<tr>
<td>Option C</td>
<td><strong>Filler Word Dictionary</strong>: Add common fillers ("ờ", "à", "uhh", "umm") to the pronunciation dictionary so MFA can align them explicitly.</td>
</tr>
<tr>
<td><strong>Recommendation</strong></td>
<td>Combine <strong>Option A</strong> (VAD masking) + <strong>Option B</strong> (post-trim) + <strong>Option C</strong> (filler dictionary).</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="2-signal-enhancement-problems">2. Signal Enhancement Problems </h2>
<h3 id="21-over-denoising-artifacts">2.1 Over-Denoising Artifacts </h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Problem</strong></td>
<td>Aggressive noise reduction can introduce "musical noise" (tonal artifacts), remove speech harmonics, or make audio sound robotic/underwater. This is especially problematic for low-SNR recordings where the boundary between speech and noise is unclear.</td>
</tr>
<tr>
<td><strong>Impact</strong></td>
<td>Enhanced audio sounds unnatural; may hurt ASR/ST model training if artifacts are consistent patterns.</td>
</tr>
<tr>
<td><strong>Proposed Solutions</strong></td>
<td></td>
</tr>
<tr>
<td>Option A</td>
<td><strong>Conservative Default</strong>: Use DeepFilterNet with lower enhancement strength (e.g., <code>--atten-lim 12</code> instead of default 100). Accept some residual noise.</td>
</tr>
<tr>
<td>Option B</td>
<td><strong>SNR-Adaptive Processing</strong>: Estimate input SNR first; only apply heavy denoising to low-SNR (&lt;10dB) files. Skip or light-touch high-SNR files.</td>
</tr>
<tr>
<td>Option C</td>
<td><strong>A/B Storage</strong>: Store both original and enhanced audio; let downstream tasks choose. Include <code>enhancement_strength</code> in metadata.</td>
</tr>
<tr>
<td><strong>Recommendation</strong></td>
<td><strong>Option B</strong> (SNR-adaptive) with <strong>Option C</strong> (dual storage) for traceability.</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="22-demucs-computational-cost">2.2 Demucs Computational Cost </h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Problem</strong></td>
<td>Demucs (music source separation) is GPU-intensive and processes at ~0.5x real-time even on modern GPUs. Processing hundreds of hours of "music" or "reaction" videos is expensive.</td>
</tr>
<tr>
<td><strong>Impact</strong></td>
<td>Pipeline bottleneck. High cloud compute costs if running on GPU instances.</td>
</tr>
<tr>
<td><strong>Proposed Solutions</strong></td>
<td></td>
</tr>
<tr>
<td>Option A</td>
<td><strong>Strict Filtering</strong>: Only route videos with explicit <code>music</code> category tag to Demucs. Use DeepFilterNet for everything else, including reactions without background music.</td>
</tr>
<tr>
<td>Option B</td>
<td><strong>Music Detection Pre-Filter</strong>: Run a lightweight music detector (e.g., <code>musicnn</code> or simple spectral analysis) to identify segments with actual background music before routing to Demucs.</td>
</tr>
<tr>
<td>Option C</td>
<td><strong>Demucs Lite</strong>: Use the smaller <code>htdemucs</code> model instead of <code>htdemucs_ft</code> for faster processing with acceptable quality trade-off.</td>
</tr>
<tr>
<td><strong>Recommendation</strong></td>
<td><strong>Option B</strong> (music detection) + <strong>Option C</strong> (lighter model). Only use Demucs when music is confirmed present.</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="23-enhancement-destroying-code-switching-cues">2.3 Enhancement Destroying Code-Switching Cues </h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Problem</strong></td>
<td>Some acoustic cues that distinguish Vietnamese from English (e.g., tonal contours for Vietnamese, certain English phonemes) might be subtly affected by enhancement models trained primarily on English/Western speech.</td>
</tr>
<tr>
<td><strong>Impact</strong></td>
<td>Hypothetically, enhanced audio could lose subtle CS markers. Needs empirical validation.</td>
</tr>
<tr>
<td><strong>Proposed Solutions</strong></td>
<td></td>
</tr>
<tr>
<td>Option A</td>
<td><strong>Empirical Testing</strong>: Run a small-scale experiment comparing ASR WER on original vs enhanced CS audio. If WER increases, investigate.</td>
</tr>
<tr>
<td>Option B</td>
<td><strong>Vietnamese-Trained Enhancer</strong>: If issue is confirmed, fine-tune DeepFilterNet on Vietnamese speech data.</td>
</tr>
<tr>
<td>Option C</td>
<td><strong>Skip Enhancement for High-Quality Sources</strong>: If original SNR is already good (&gt;20dB), skip enhancement entirely.</td>
</tr>
<tr>
<td><strong>Recommendation</strong></td>
<td>Start with <strong>Option A</strong> (empirical test) before assuming this is a real problem.</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="24-handling-already-processed-audio">2.4 Handling Already-Processed Audio </h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Problem</strong></td>
<td>Some YouTube videos are already post-processed by creators (noise-gated, compressed, EQ'd). Applying DeepFilterNet on top may cause double-processing artifacts or actually degrade quality.</td>
</tr>
<tr>
<td><strong>Impact</strong></td>
<td>Inconsistent audio quality across dataset.</td>
</tr>
<tr>
<td><strong>Proposed Solutions</strong></td>
<td></td>
</tr>
<tr>
<td>Option A</td>
<td><strong>Quality Detection</strong>: Analyze spectral characteristics to detect already-processed audio (e.g., sharp high-frequency rolloff indicating compression). Skip enhancement for these.</td>
</tr>
<tr>
<td>Option B</td>
<td><strong>SNR Threshold</strong>: If estimated SNR is already &gt;25dB, assume audio is clean enough and skip enhancement.</td>
</tr>
<tr>
<td>Option C</td>
<td><strong>Metadata Heuristic</strong>: Professional channels (high subscriber count, verified) likely have good audio. Use this as a soft signal to skip enhancement.</td>
</tr>
<tr>
<td><strong>Recommendation</strong></td>
<td><strong>Option B</strong> (SNR threshold) is simplest and effective.</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="3-tts-synthesis-problems">3. TTS Synthesis Problems </h2>
<h3 id="31-word-level-language-identification-accuracy">3.1 Word-Level Language Identification Accuracy </h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Problem</strong></td>
<td>To insert <code>[VI]</code>/<code>[EN]</code> tags, we need word-level LID. But short words are ambiguous: "OK" could be English or Vietnamese internet slang, "không" is Vietnamese but might appear in English contexts as a loanword. fastText/langdetect have high error rates on single words.</td>
</tr>
<tr>
<td><strong>Impact</strong></td>
<td>Incorrect tags cause TTS to use wrong phonemization, producing unnatural speech.</td>
</tr>
<tr>
<td><strong>Proposed Solutions</strong></td>
<td></td>
</tr>
<tr>
<td>Option A</td>
<td><strong>Phrase-Level LID</strong>: Instead of word-level, detect language at phrase/clause level (3-5 words). More context = better accuracy. Insert tags at phrase boundaries.</td>
</tr>
<tr>
<td>Option B</td>
<td><strong>Dictionary Lookup + LID Fallback</strong>: Maintain a Vietnamese word list and English word list. Use dictionary lookup first; only use LID model for unknown words.</td>
</tr>
<tr>
<td>Option C</td>
<td><strong>Character-Based Heuristics</strong>: Vietnamese uses diacritics (ă, â, ê, ô, ơ, ư, đ). If a word contains these, it's Vietnamese. If purely ASCII with common English patterns, assume English. Only use LID for edge cases.</td>
</tr>
<tr>
<td>Option D</td>
<td><strong>Manual Annotation</strong>: For high-quality synthetic data, have annotators tag language spans manually in Label Studio.</td>
</tr>
<tr>
<td><strong>Recommendation</strong></td>
<td><strong>Option C</strong> (character heuristics) + <strong>Option A</strong> (phrase-level) as fallback. Use <strong>Option D</strong> (manual) for gold-standard synthetic data.</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="32-intra-sentence-tts-switching-artifacts">3.2 Intra-Sentence TTS Switching Artifacts </h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Problem</strong></td>
<td>XTTS v2 generates one language per call. To produce CS audio, we must: (1) split text into language spans, (2) synthesize each span separately, (3) concatenate audio. This introduces audible discontinuities: pitch jumps, timing mismatches, unnatural pauses, or abrupt prosody changes at boundaries.</td>
</tr>
<tr>
<td><strong>Impact</strong></td>
<td>Synthetic CS audio sounds robotic and unnatural, unlike real CS speakers who smoothly blend languages.</td>
</tr>
<tr>
<td><strong>Proposed Solutions</strong></td>
<td></td>
</tr>
<tr>
<td>Option A</td>
<td><strong>Crossfade Stitching</strong>: Apply 50-100ms crossfade at boundaries to smooth transitions. Simple but doesn't fix prosody mismatches.</td>
</tr>
<tr>
<td>Option B</td>
<td><strong>Prosody Conditioning</strong>: Extract F0 (pitch) and duration from the previous span's last phoneme, use it to condition the next span's first phoneme. Requires model modification.</td>
</tr>
<tr>
<td>Option C</td>
<td><strong>End-to-End CS TTS Model</strong>: Train or fine-tune a TTS model that natively handles CS input (e.g., fine-tune XTTS on real CS data). This is the "correct" solution but requires significant effort.</td>
</tr>
<tr>
<td>Option D</td>
<td><strong>Voice Cloning Consistency</strong>: Use the same voice prompt for all spans in a sentence to maintain speaker consistency. Still has prosody issues but reduces voice mismatch.</td>
</tr>
<tr>
<td><strong>Recommendation</strong></td>
<td>Short-term: <strong>Option A</strong> (crossfade) + <strong>Option D</strong> (consistent voice). Long-term: Explore <strong>Option C</strong> (CS-native TTS).</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="33-tts-model-availability-and-access">3.3 TTS Model Availability and Access </h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Problem</strong></td>
<td>High-quality CS-capable TTS models have limited availability: (1) <strong>VALL-E X</strong> is a research paper from Microsoft with no official open-source release; unofficial implementations lack Vietnamese support. (2) <strong>F5-TTS</strong> is a promising diffusion-based model with excellent zero-shot voice cloning and potential CS capabilities, but we currently <strong>do not have access</strong> to the model weights.</td>
</tr>
<tr>
<td><strong>Impact</strong></td>
<td>Cannot reliably use VALL-E X or F5-TTS as proposed. Limited to XTTS v2 and MMS-TTS, which have CS stitching limitations.</td>
</tr>
<tr>
<td><strong>Proposed Solutions</strong></td>
<td></td>
</tr>
<tr>
<td>Option A</td>
<td><strong>Drop VALL-E X, Use XTTS v2 Only</strong>: Focus solely on XTTS v2, which has official support and active maintenance. Accept CS stitching artifacts.</td>
</tr>
<tr>
<td>Option B</td>
<td><strong>Substitute with MMS-TTS</strong>: Meta's Massively Multilingual Speech TTS supports Vietnamese (<code>vie</code>) and English (<code>eng</code>). Simpler API, consistent quality, but less expressive than XTTS.</td>
</tr>
<tr>
<td>Option C</td>
<td><strong>Contact F5-TTS Authors</strong>: Email the research team to request model access for academic/research use. F5-TTS's flow-matching approach may handle CS better than autoregressive models.</td>
</tr>
<tr>
<td>Option D</td>
<td><strong>Fine-tune F5-TTS on YouTube Data</strong>: If base F5-TTS becomes available (or using open checkpoints), fine-tune on our segmented YouTube CS audio to create a Vietnamese-English CS-native TTS. Requires significant compute and aligned data.</td>
</tr>
<tr>
<td>Option E</td>
<td><strong>Abandon Text-First Pipeline</strong>: If TTS quality is insufficient, <strong>drop Substack ingestion entirely</strong> and focus only on YouTube (Audio-First pipeline). Avoids TTS complexity at the cost of losing text-sourced CS data diversity.</td>
</tr>
<tr>
<td><strong>Recommendation</strong></td>
<td>Short-term: <strong>Option A</strong> (XTTS v2) + <strong>Option B</strong> (MMS fallback). Parallel: Pursue <strong>Option C</strong> (contact F5-TTS authors). Fallback: <strong>Option E</strong> (drop Substack) if TTS proves intractable.</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="34-vietnamese-phoneme-coverage-in-tts">3.4 Vietnamese Phoneme Coverage in TTS </h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Problem</strong></td>
<td>Vietnamese has 6 tones and complex vowel combinations. Some TTS models (especially English-primary ones) have incomplete Vietnamese phoneme coverage, leading to mispronunciations of tonal words or rare diacritics.</td>
</tr>
<tr>
<td><strong>Impact</strong></td>
<td>Synthetic Vietnamese sounds foreign or incorrect, reducing training data quality.</td>
</tr>
<tr>
<td><strong>Proposed Solutions</strong></td>
<td></td>
</tr>
<tr>
<td>Option A</td>
<td><strong>Use ViXTTS</strong>: Community fine-tuned XTTS specifically for Vietnamese. Better phoneme coverage than base XTTS.</td>
</tr>
<tr>
<td>Option B</td>
<td><strong>VITS-Based Vietnamese TTS</strong>: Use a dedicated Vietnamese TTS model (e.g., <code>vietTTS</code>, <code>vinai/vits-vi</code>) for Vietnamese spans, XTTS for English spans.</td>
</tr>
<tr>
<td>Option C</td>
<td><strong>Phoneme Normalization</strong>: Pre-process Vietnamese text to normalize rare characters and add explicit tone markers that TTS models handle better.</td>
</tr>
<tr>
<td><strong>Recommendation</strong></td>
<td><strong>Option B</strong> (dedicated Vietnamese TTS for Vietnamese spans) provides best quality. Evaluate ViXTTS (<strong>Option A</strong>) as a unified alternative.</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="35-speaker-diversity-in-synthetic-data">3.5 Speaker Diversity in Synthetic Data </h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Problem</strong></td>
<td>TTS with a single voice prompt produces monotonous data. Real CS speech has diverse speakers (age, gender, accent, speaking rate). Synthetic data lacking diversity may cause model overfitting to TTS voice characteristics.</td>
</tr>
<tr>
<td><strong>Impact</strong></td>
<td>Models trained on synthetic data may not generalize to real speakers.</td>
</tr>
<tr>
<td><strong>Proposed Solutions</strong></td>
<td></td>
</tr>
<tr>
<td>Option A</td>
<td><strong>Voice Prompt Bank</strong>: Collect 50-100 diverse voice prompts (6-second clips from real Vietnamese-English speakers). Randomly select prompts for each synthesis.</td>
</tr>
<tr>
<td>Option B</td>
<td><strong>Voice Augmentation</strong>: Apply pitch shifting, speed variation, and formant modification to synthetic audio to simulate speaker diversity.</td>
</tr>
<tr>
<td>Option C</td>
<td><strong>Multi-Model Ensemble</strong>: Use multiple TTS models (XTTS, MMS-TTS, VITS) to generate the same text, creating natural variation.</td>
</tr>
<tr>
<td><strong>Recommendation</strong></td>
<td><strong>Option A</strong> (voice bank) is most realistic. Supplement with <strong>Option B</strong> (augmentation) for further diversity.</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="36-text-normalization-for-tts-input">3.6 Text Normalization for TTS Input </h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Problem</strong></td>
<td>CS text from blogs contains: URLs, emails, numbers, dates, abbreviations, emoji, teencode (Vietnamese internet slang). TTS models cannot directly speak these; they need expansion (e.g., "5k" → "năm nghìn", "ASAP" → "as soon as possible").</td>
</tr>
<tr>
<td><strong>Impact</strong></td>
<td>TTS produces garbage audio for unnormalized tokens or speaks them letter-by-letter.</td>
</tr>
<tr>
<td><strong>Proposed Solutions</strong></td>
<td></td>
</tr>
<tr>
<td>Option A</td>
<td><strong>Rule-Based Normalizer</strong>: Build a normalizer with rules for numbers (Vietnamese vs English reading), dates, common abbreviations, URLs (skip or say "link").</td>
</tr>
<tr>
<td>Option B</td>
<td><strong>Leverage text_utils.py</strong>: Extend existing <code>normalize_text()</code> to handle TTS-specific expansions. The teencode dictionary already exists.</td>
</tr>
<tr>
<td>Option C</td>
<td><strong>LLM Normalization</strong>: Use an LLM to rewrite text into speakable form. Handles edge cases but slow and expensive.</td>
</tr>
<tr>
<td><strong>Recommendation</strong></td>
<td><strong>Option B</strong> (extend existing utils) + <strong>Option A</strong> (rule-based) for common patterns. Reserve <strong>Option C</strong> (LLM) for complex edge cases.</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="4-cross-cutting-infrastructure-problems">4. Cross-Cutting Infrastructure Problems </h2>
<h3 id="41-pipeline-orchestration-and-failure-recovery">4.1 Pipeline Orchestration and Failure Recovery </h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Problem</strong></td>
<td>Preprocessing involves multiple sequential stages (VAD → MFA → Segment → Enhance → LID → TTS). If one stage fails mid-batch, we need to: (1) identify which samples failed, (2) resume from failure point, (3) avoid reprocessing successful samples.</td>
</tr>
<tr>
<td><strong>Impact</strong></td>
<td>Manual recovery is error-prone and wastes compute.</td>
</tr>
<tr>
<td><strong>Proposed Solutions</strong></td>
<td></td>
</tr>
<tr>
<td>Option A</td>
<td><strong>Database State Machine</strong>: Use existing <code>processing_state</code> enum to track each sample's progress. Each script queries for samples in the appropriate input state, processes them, updates state on success. Failures remain in original state for retry.</td>
</tr>
<tr>
<td>Option B</td>
<td><strong>DVC Pipeline with Checkpoints</strong>: Use DVC stages with proper <code>deps</code> and <code>outs</code>. DVC handles caching and skips completed stages.</td>
</tr>
<tr>
<td>Option C</td>
<td><strong>Task Queue (Celery/RQ)</strong>: Use a proper task queue for distributed processing. Each sample is a task; failed tasks go to dead-letter queue for retry.</td>
</tr>
<tr>
<td><strong>Recommendation</strong></td>
<td><strong>Option A</strong> (database state machine) for simplicity—already partially implemented. Add <strong>Option B</strong> (DVC) for batch-level orchestration.</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="42-storage-growth-and-data-versioning">4.2 Storage Growth and Data Versioning </h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Problem</strong></td>
<td>Each preprocessing stage creates new files: aligned TextGrids, segmented audio clips (many small files per video), enhanced audio, synthetic audio. Storage grows multiplicatively. Versioning all artifacts is expensive.</td>
</tr>
<tr>
<td><strong>Impact</strong></td>
<td>Disk space exhaustion. Slow DVC pushes. Difficulty tracking which version of enhanced data was used for training.</td>
</tr>
<tr>
<td><strong>Proposed Solutions</strong></td>
<td></td>
</tr>
<tr>
<td>Option A</td>
<td><strong>Selective DVC Tracking</strong>: Only track final outputs (<code>data/reviewed/</code>) with DVC. Intermediate stages are reproducible from raw data + code.</td>
</tr>
<tr>
<td>Option B</td>
<td><strong>Compressed Intermediate Storage</strong>: Store segmented audio as compressed archives (<code>.tar.gz</code>) rather than loose files. Reduces file count and improves transfer speeds.</td>
</tr>
<tr>
<td>Option C</td>
<td><strong>Cloud Tiering</strong>: Move older/intermediate data to cold storage (e.g., S3 Glacier). Keep only active data on hot storage.</td>
</tr>
<tr>
<td><strong>Recommendation</strong></td>
<td><strong>Option A</strong> (selective tracking) + <strong>Option B</strong> (compressed intermediates).</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="43-compute-resource-management">4.3 Compute Resource Management </h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Problem</strong></td>
<td>Different stages have different compute profiles: MFA is CPU-bound (multi-core), DeepFilterNet is CPU-efficient, Demucs needs GPU, TTS needs GPU. Running everything on one machine is inefficient.</td>
</tr>
<tr>
<td><strong>Impact</strong></td>
<td>Underutilized resources. Slow batch processing.</td>
</tr>
<tr>
<td><strong>Proposed Solutions</strong></td>
<td></td>
</tr>
<tr>
<td>Option A</td>
<td><strong>Stage-Specific Workers</strong>: Deploy CPU workers for MFA/DeepFilterNet, GPU workers for Demucs/TTS. Use task queue to route jobs appropriately.</td>
</tr>
<tr>
<td>Option B</td>
<td><strong>Sequential Batch Processing</strong>: Process all samples through CPU stages first, then batch all GPU work together to maximize GPU utilization.</td>
</tr>
<tr>
<td>Option C</td>
<td><strong>Cloud Burst</strong>: Run GPU stages on cloud (Lambda Labs, RunPod) only when needed. Keep CPU stages on local/cheap infrastructure.</td>
</tr>
<tr>
<td><strong>Recommendation</strong></td>
<td>Start with <strong>Option B</strong> (sequential batching). Scale to <strong>Option A</strong> (specialized workers) if throughput is insufficient.</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="44-configuration-management">4.4 Configuration Management </h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Problem</strong></td>
<td>Preprocessing involves many hyperparameters: VAD threshold, MFA beam width, segment min/max length, SNR threshold for enhancement, LID confidence threshold, TTS voice prompts. These need to be: (1) versioned, (2) reproducible, (3) easily tunable.</td>
</tr>
<tr>
<td><strong>Impact</strong></td>
<td>Experiments are not reproducible. Hard to compare results across different parameter settings.</td>
</tr>
<tr>
<td><strong>Proposed Solutions</strong></td>
<td></td>
</tr>
<tr>
<td>Option A</td>
<td><strong>Config File</strong>: Single <code>config/preprocessing.yaml</code> with all parameters. Load at runtime. Version with git.</td>
</tr>
<tr>
<td>Option B</td>
<td><strong>Environment Variables</strong>: Use <code>.env</code> for deployment-specific settings, config file for algorithm parameters.</td>
</tr>
<tr>
<td>Option C</td>
<td><strong>Hydra/OmegaConf</strong>: Use a configuration framework that supports hierarchical configs, overrides, and experiment tracking.</td>
</tr>
<tr>
<td><strong>Recommendation</strong></td>
<td><strong>Option A</strong> (YAML config) for simplicity. Upgrade to <strong>Option C</strong> (Hydra) if experimentation becomes complex.</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="45-monitoring-and-logging">4.5 Monitoring and Logging </h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Problem</strong></td>
<td>Long-running preprocessing jobs need observability: progress tracking, error rates, resource utilization, quality metrics (SNR before/after, alignment confidence).</td>
</tr>
<tr>
<td><strong>Impact</strong></td>
<td>Blind spots in pipeline health. Difficult to diagnose issues.</td>
</tr>
<tr>
<td><strong>Proposed Solutions</strong></td>
<td></td>
</tr>
<tr>
<td>Option A</td>
<td><strong>Structured Logging</strong>: Use Python <code>logging</code> with JSON formatter. Include sample_id, stage, metrics in every log line. Aggregate with ELK/Loki.</td>
</tr>
<tr>
<td>Option B</td>
<td><strong>Database Metrics Table</strong>: Store per-sample metrics (processing time, SNR delta, alignment score) in a <code>preprocessing_metrics</code> table. Query for dashboards.</td>
</tr>
<tr>
<td>Option C</td>
<td><strong>Progress Bars + Notifications</strong>: Use <code>tqdm</code> for local runs, send Slack/Discord webhooks on batch completion or failure.</td>
</tr>
<tr>
<td><strong>Recommendation</strong></td>
<td><strong>Option A</strong> (structured logging) + <strong>Option B</strong> (database metrics) + <strong>Option C</strong> (tqdm for UX).</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="5-data-quality--validation-problems">5. Data Quality &amp; Validation Problems </h2>
<h3 id="51-segment-transcript-alignment-validation">5.1 Segment-Transcript Alignment Validation </h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Problem</strong></td>
<td>After segmentation, how do we verify that the audio segment actually matches its transcript? MFA can produce alignments that are technically "complete" but semantically wrong (words mapped to wrong audio regions).</td>
</tr>
<tr>
<td><strong>Impact</strong></td>
<td>Garbage training data that looks valid.</td>
</tr>
<tr>
<td><strong>Proposed Solutions</strong></td>
<td></td>
</tr>
<tr>
<td>Option A</td>
<td><strong>ASR Cross-Check</strong>: Run Whisper on each segment, compare ASR output to transcript using WER/CER. Flag segments with high error rate for review.</td>
</tr>
<tr>
<td>Option B</td>
<td><strong>Alignment Confidence Scores</strong>: MFA outputs log-likelihood scores. Filter out low-confidence alignments.</td>
</tr>
<tr>
<td>Option C</td>
<td><strong>Human Sampling</strong>: Randomly sample 5% of segments for human review in Label Studio. Use error rate to estimate overall quality.</td>
</tr>
<tr>
<td><strong>Recommendation</strong></td>
<td><strong>Option B</strong> (confidence filtering) as automated gate. <strong>Option A</strong> (ASR cross-check) for flagged samples. <strong>Option C</strong> (human sampling) for validation.</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="52-cs-content-verification">5.2 CS Content Verification </h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Problem</strong></td>
<td>Not all segments contain actual code-switching. Some may be monolingual Vietnamese or English. The pipeline should identify and tag true CS segments vs. monolingual segments.</td>
</tr>
<tr>
<td><strong>Impact</strong></td>
<td>Diluted CS training data if monolingual segments are not filtered or labeled.</td>
</tr>
<tr>
<td><strong>Proposed Solutions</strong></td>
<td></td>
</tr>
<tr>
<td>Option A</td>
<td><strong>Reuse <code>detect_cs_text()</code></strong>: Apply existing CS detection from <code>text_utils.py</code> to segment transcripts. Tag each segment with <code>is_cs: true/false</code>.</td>
</tr>
<tr>
<td>Option B</td>
<td><strong>CS Ratio Threshold</strong>: Compute CS ratio (Vietnamese/English word mix) per segment. Only label as CS if ratio is between 10%-90% (true mix, not occasional loanword).</td>
</tr>
<tr>
<td>Option C</td>
<td><strong>Separate Datasets</strong>: Split segments into <code>cs_samples</code>, <code>vi_samples</code>, <code>en_samples</code> tables/folders for different training objectives.</td>
</tr>
<tr>
<td><strong>Recommendation</strong></td>
<td><strong>Option A</strong> + <strong>Option B</strong> (detect + threshold) to tag segments. Consider <strong>Option C</strong> if training separate models.</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="53-audio-quality-metrics">5.3 Audio Quality Metrics </h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Problem</strong></td>
<td>We need objective measures of audio quality to: (1) decide if enhancement is needed, (2) verify enhancement improved quality, (3) filter out unsalvageable samples. Metrics: SNR, PESQ, STOI, clipping ratio.</td>
</tr>
<tr>
<td><strong>Impact</strong></td>
<td>Cannot make data-driven decisions about audio processing.</td>
</tr>
<tr>
<td><strong>Proposed Solutions</strong></td>
<td></td>
</tr>
<tr>
<td>Option A</td>
<td><strong>SNR Estimation</strong>: Use <code>waveform_analysis</code> or <code>pyloudnorm</code> to estimate SNR. Store in <code>audio_meta.snr_db</code>.</td>
</tr>
<tr>
<td>Option B</td>
<td><strong>PESQ/STOI (Reference-Based)</strong>: These require clean reference audio, which we don't have for real data. Only usable for synthetic data quality.</td>
</tr>
<tr>
<td>Option C</td>
<td><strong>DNSMOS (Non-Intrusive)</strong>: Microsoft's DNSMOS predicts MOS score without reference. Good for real data.</td>
</tr>
<tr>
<td><strong>Recommendation</strong></td>
<td><strong>Option A</strong> (SNR) for all audio. <strong>Option C</strong> (DNSMOS) for quality scoring. Reserve <strong>Option B</strong> for synthetic data validation.</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="54-handling-profanity-sensitive-content">5.4 Handling Profanity, Sensitive Content </h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Problem</strong></td>
<td>Vietnamese vlogs may contain profanity, sensitive topics, or copyrighted music. These need to be flagged for review or exclusion depending on intended use of the model.</td>
</tr>
<tr>
<td><strong>Impact</strong></td>
<td>Legal/ethical issues with training data.</td>
</tr>
<tr>
<td><strong>Proposed Solutions</strong></td>
<td></td>
</tr>
<tr>
<td>Option A</td>
<td><strong>Keyword Filter</strong>: Maintain a list of profanity/sensitive keywords in Vietnamese and English. Flag segments containing matches.</td>
</tr>
<tr>
<td>Option B</td>
<td><strong>Content Classification Model</strong>: Use a text classifier to detect sensitive content categories.</td>
</tr>
<tr>
<td>Option C</td>
<td><strong>Manual Review Tags</strong>: Add <code>content_warning</code> field to samples. Populate during Label Studio review.</td>
</tr>
<tr>
<td><strong>Recommendation</strong></td>
<td><strong>Option A</strong> (keyword filter) for automated flagging. <strong>Option C</strong> (manual tags) for nuanced cases.</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="6-additional-workflow-problems">6. Additional Workflow Problems </h2>
<h3 id="61-data-licensing-and-copyright">6.1 Data Licensing and Copyright </h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Problem</strong></td>
<td>YouTube videos and Substack articles are copyrighted content. Using them for model training may violate terms of service or copyright law depending on jurisdiction and intended use (research vs. commercial).</td>
</tr>
<tr>
<td><strong>Impact</strong></td>
<td>Legal risk. Inability to release trained models or datasets publicly.</td>
</tr>
<tr>
<td><strong>Proposed Solutions</strong></td>
<td></td>
</tr>
<tr>
<td>Option A</td>
<td><strong>Research-Only Use</strong>: Restrict dataset and models to internal research. Do not publish dataset; only publish model trained on it with appropriate disclaimers.</td>
</tr>
<tr>
<td>Option B</td>
<td><strong>Fair Use Documentation</strong>: Document that usage falls under fair use/research exemption. Keep records of sources for potential takedown requests.</td>
</tr>
<tr>
<td>Option C</td>
<td><strong>Seek Explicit Permission</strong>: For high-value channels, contact creators for permission to use their content.</td>
</tr>
<tr>
<td>Option D</td>
<td><strong>Synthetic-Only Public Release</strong>: Train on real data, but only release models fine-tuned on fully synthetic data for public use.</td>
</tr>
<tr>
<td><strong>Recommendation</strong></td>
<td><strong>Option A</strong> (research-only) + <strong>Option B</strong> (fair use docs). Consider <strong>Option D</strong> for any public release.</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="62-youtube-api-rate-limits-and-account-bans">6.2 YouTube API Rate Limits and Account Bans </h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Problem</strong></td>
<td>Bulk downloading videos with <code>yt-dlp</code> and transcripts with <code>youtube-transcript-api</code> can trigger rate limiting or account/IP bans from YouTube. This disrupts data collection and may permanently block access.</td>
</tr>
<tr>
<td><strong>Impact</strong></td>
<td>Data collection halts. Need to rotate IPs or wait for cooldown.</td>
</tr>
<tr>
<td><strong>Proposed Solutions</strong></td>
<td></td>
</tr>
<tr>
<td>Option A</td>
<td><strong>Throttling</strong>: Add random delays (5-30s) between downloads. Limit to 50-100 videos per day per IP.</td>
</tr>
<tr>
<td>Option B</td>
<td><strong>Proxy Rotation</strong>: Use rotating residential proxies to distribute requests across IPs.</td>
</tr>
<tr>
<td>Option C</td>
<td><strong>Batch Scheduling</strong>: Download in small batches overnight during off-peak hours.</td>
</tr>
<tr>
<td>Option D</td>
<td><strong><a href="http://Archive.org">Archive.org</a> Fallback</strong>: Check if target videos exist on Internet Archive before hitting YouTube.</td>
</tr>
<tr>
<td><strong>Recommendation</strong></td>
<td><strong>Option A</strong> (throttling) + <strong>Option C</strong> (batch scheduling) for sustainable collection.</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="63-model-dependency-version-conflicts">6.3 Model Dependency Version Conflicts </h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Problem</strong></td>
<td>The preprocessing pipeline uses multiple ML models (Whisper, MFA, DeepFilterNet, Demucs, XTTS, pyannote) with conflicting dependencies. For example, XTTS requires specific PyTorch versions; MFA requires Conda; pyannote requires <code>pyannote.audio</code> with its own dependency tree.</td>
</tr>
<tr>
<td><strong>Impact</strong></td>
<td>Dependency hell. Installation failures. Runtime errors from version mismatches.</td>
</tr>
<tr>
<td><strong>Proposed Solutions</strong></td>
<td></td>
</tr>
<tr>
<td>Option A</td>
<td><strong>Separate Virtual Environments</strong>: Use different venvs/conda envs for each tool. Orchestrate via subprocess calls.</td>
</tr>
<tr>
<td>Option B</td>
<td><strong>Containerization</strong>: Create separate Docker containers for each tool. Pipeline calls containers via CLI or API.</td>
</tr>
<tr>
<td>Option C</td>
<td><strong>Careful Pinning</strong>: Exhaustively test compatible versions and pin in <code>requirements.txt</code>. May not always be possible.</td>
</tr>
<tr>
<td>Option D</td>
<td><strong>Microservice Architecture</strong>: Deploy heavy models (TTS, Demucs) as separate services with REST APIs. Main pipeline calls APIs.</td>
</tr>
<tr>
<td><strong>Recommendation</strong></td>
<td><strong>Option B</strong> (Docker containers) for isolation. Start with <strong>Option A</strong> (separate envs) for local development.</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="64-transcript-language-mismatch">6.4 Transcript Language Mismatch </h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Problem</strong></td>
<td>YouTube transcript language metadata can be wrong. A video marked as "Vietnamese" transcript may actually be in English, or vice versa. Auto-translated transcripts are especially unreliable.</td>
</tr>
<tr>
<td><strong>Impact</strong></td>
<td>Wrong transcript language breaks downstream processing (MFA dictionary selection, LID assumptions).</td>
</tr>
<tr>
<td><strong>Proposed Solutions</strong></td>
<td></td>
</tr>
<tr>
<td>Option A</td>
<td><strong>LID Verification</strong>: Run language detection on the full transcript text. If detected language differs from metadata, flag for review or reassign.</td>
</tr>
<tr>
<td>Option B</td>
<td><strong>Prefer Manual Over Auto</strong>: Already implemented—prioritize manual captions. Auto-generated captions are more likely to have language issues.</td>
</tr>
<tr>
<td>Option C</td>
<td><strong>Multi-Language Detection</strong>: For CS content, expect mixed results from LID. Accept if LID returns both <code>vi</code> and <code>en</code> with significant confidence.</td>
</tr>
<tr>
<td><strong>Recommendation</strong></td>
<td><strong>Option A</strong> (LID verify) + <strong>Option B</strong> (prefer manual).</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="65-audio-transcript-temporal-alignment-for-long-videos">6.5 Audio-Transcript Temporal Alignment for Long Videos </h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Problem</strong></td>
<td>For videos &gt;30 minutes, cumulative drift between audio and transcript timestamps can exceed 5-10 seconds by the end. MFA may fail to recover from such large offsets.</td>
</tr>
<tr>
<td><strong>Impact</strong></td>
<td>Later portions of long videos have unusable alignments.</td>
</tr>
<tr>
<td><strong>Proposed Solutions</strong></td>
<td></td>
</tr>
<tr>
<td>Option A</td>
<td><strong>Anchor Point Detection</strong>: Identify clear alignment anchors (e.g., chapter markers, scene changes, distinct phrases) and re-sync at these points.</td>
</tr>
<tr>
<td>Option B</td>
<td><strong>Sliding Window Alignment</strong>: Process in 2-3 minute windows with overlap. Stitch results, using overlap to detect and correct drift.</td>
</tr>
<tr>
<td>Option C</td>
<td><strong>Limit Video Length</strong>: Only process videos &lt;20 minutes where drift is manageable. Skip or split longer videos.</td>
</tr>
<tr>
<td><strong>Recommendation</strong></td>
<td><strong>Option B</strong> (sliding window) is most robust. Apply <strong>Option C</strong> (length limit) as initial filter.</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="66-handling-incomplete-or-corrupted-downloads">6.6 Handling Incomplete or Corrupted Downloads </h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Problem</strong></td>
<td>Network interruptions can leave partial audio files or truncated transcripts. These corrupt files may not fail obviously—they might load but have missing data at the end.</td>
</tr>
<tr>
<td><strong>Impact</strong></td>
<td>Silent data corruption. Downstream processing produces incorrect results without error.</td>
</tr>
<tr>
<td><strong>Proposed Solutions</strong></td>
<td></td>
</tr>
<tr>
<td>Option A</td>
<td><strong>Checksum Validation</strong>: After download, verify file integrity (check audio duration matches metadata, transcript segment count is reasonable).</td>
</tr>
<tr>
<td>Option B</td>
<td><strong>Re-download on Mismatch</strong>: If validation fails, delete and re-download with retry logic.</td>
</tr>
<tr>
<td>Option C</td>
<td><strong>Quarantine Directory</strong>: Move suspicious files to a quarantine folder for manual review rather than processing them.</td>
</tr>
<tr>
<td><strong>Recommendation</strong></td>
<td><strong>Option A</strong> (validation) + <strong>Option B</strong> (retry) + <strong>Option C</strong> (quarantine) for defense in depth.</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="67-teencode-and-slang-evolution">6.7 Teencode and Slang Evolution </h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Problem</strong></td>
<td>Vietnamese internet slang (teencode) evolves rapidly. The static <code>teencode.txt</code> dictionary will become outdated. New slang terms won't be normalized, affecting TTS pronunciation and text consistency.</td>
</tr>
<tr>
<td><strong>Impact</strong></td>
<td>Degraded text normalization over time. TTS mispronounces new slang.</td>
</tr>
<tr>
<td><strong>Proposed Solutions</strong></td>
<td></td>
</tr>
<tr>
<td>Option A</td>
<td><strong>Periodic Dictionary Updates</strong>: Schedule quarterly reviews of teencode dictionary. Add new terms from recent data.</td>
</tr>
<tr>
<td>Option B</td>
<td><strong>Crowdsourced Updates</strong>: Allow annotators in Label Studio to flag unknown slang terms. Batch add to dictionary.</td>
</tr>
<tr>
<td>Option C</td>
<td><strong>LLM-Based Normalization</strong>: Use an LLM to detect and expand unknown slang contextually. More robust to new terms but slower.</td>
</tr>
<tr>
<td><strong>Recommendation</strong></td>
<td><strong>Option B</strong> (crowdsourced) for ongoing maintenance. <strong>Option A</strong> (periodic review) as baseline.</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="68-voice-prompt-quality-for-tts">6.8 Voice Prompt Quality for TTS </h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Problem</strong></td>
<td>XTTS v2 voice cloning requires a 6-second clean audio prompt. If prompts are noisy, have background music, or contain multiple speakers, the cloned voice quality degrades significantly.</td>
</tr>
<tr>
<td><strong>Impact</strong></td>
<td>Synthetic audio has artifacts, wrong speaker characteristics, or unintelligible output.</td>
</tr>
<tr>
<td><strong>Proposed Solutions</strong></td>
<td></td>
</tr>
<tr>
<td>Option A</td>
<td><strong>Curated Prompt Bank</strong>: Manually select and validate high-quality 6-second clips from our YouTube data. Filter for: single speaker, low noise, clear speech.</td>
</tr>
<tr>
<td>Option B</td>
<td><strong>Enhancement Before Prompting</strong>: Run DeepFilterNet on candidate prompts before using them for voice cloning.</td>
</tr>
<tr>
<td>Option C</td>
<td><strong>Synthetic Prompt Verification</strong>: Generate a test phrase with each prompt, manually verify quality before adding to bank.</td>
</tr>
<tr>
<td><strong>Recommendation</strong></td>
<td><strong>Option A</strong> (curated bank) + <strong>Option C</strong> (verification) for quality assurance.</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="69-gpu-memory-management-for-batch-processing">6.9 GPU Memory Management for Batch Processing </h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Problem</strong></td>
<td>Models like XTTS, Demucs, and Whisper have high GPU memory requirements (8-16GB). Processing long audio files or large batches causes OOM errors. Memory leaks from repeated model loads can accumulate.</td>
</tr>
<tr>
<td><strong>Impact</strong></td>
<td>Crashes mid-batch. Wasted compute time.</td>
</tr>
<tr>
<td><strong>Proposed Solutions</strong></td>
<td></td>
</tr>
<tr>
<td>Option A</td>
<td><strong>Chunk Long Audio</strong>: Split audio &gt;30s into smaller chunks for GPU processing. Concatenate results.</td>
</tr>
<tr>
<td>Option B</td>
<td><strong>Explicit Memory Management</strong>: Call <code>torch.cuda.empty_cache()</code> between batches. Use context managers to ensure model unloading.</td>
</tr>
<tr>
<td>Option C</td>
<td><strong>Process Isolation</strong>: Run each GPU task in a subprocess that terminates after completion, ensuring full memory release.</td>
</tr>
<tr>
<td>Option D</td>
<td><strong>Batch Size Tuning</strong>: Dynamically adjust batch size based on available GPU memory.</td>
</tr>
<tr>
<td><strong>Recommendation</strong></td>
<td><strong>Option A</strong> (chunking) + <strong>Option B</strong> (cache clearing) + <strong>Option C</strong> (subprocess isolation for long-running jobs).</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="610-reproducibility-of-preprocessing-results">6.10 Reproducibility of Preprocessing Results </h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Problem</strong></td>
<td>Some preprocessing steps have non-deterministic behavior: VAD thresholds can produce slightly different segments on re-runs, TTS generation varies with random seeds, model inference can differ across GPU architectures.</td>
</tr>
<tr>
<td><strong>Impact</strong></td>
<td>Cannot exactly reproduce a dataset. Experiments are not fully repeatable.</td>
</tr>
<tr>
<td><strong>Proposed Solutions</strong></td>
<td></td>
</tr>
<tr>
<td>Option A</td>
<td><strong>Fixed Random Seeds</strong>: Set <code>torch.manual_seed()</code>, <code>numpy.random.seed()</code>, etc., for all stochastic operations. Document seeds in metadata.</td>
</tr>
<tr>
<td>Option B</td>
<td><strong>Snapshot Outputs</strong>: Store preprocessing outputs with DVC. Re-run only regenerates if code/config changes, not on every run.</td>
</tr>
<tr>
<td>Option C</td>
<td><strong>Hash-Based Caching</strong>: Hash inputs + config; if hash matches cached result, skip reprocessing.</td>
</tr>
<tr>
<td><strong>Recommendation</strong></td>
<td><strong>Option A</strong> (fixed seeds) + <strong>Option B</strong> (DVC snapshots) for reproducibility.</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="7-decision-matrix">7. Decision Matrix </h2>
<p>Summary of key decisions to make before implementation:</p>
<table>
<thead>
<tr>
<th>Decision</th>
<th>Options</th>
<th>Effort</th>
<th>Quality Impact</th>
<th>Recommendation</th>
</tr>
</thead>
<tbody>
<tr>
<td>MFA Dictionary Strategy</td>
<td>Merged / Two-Pass / Hybrid</td>
<td>Med / Low / High</td>
<td>High / Med / High</td>
<td>Start Two-Pass</td>
</tr>
<tr>
<td>Transcript-less Videos</td>
<td>VAD / Whisper+MFA / Skip</td>
<td>Low / Med / None</td>
<td>Low / High / None</td>
<td>Whisper+MFA</td>
</tr>
<tr>
<td>Enhancement Routing</td>
<td>SNR-adaptive / Always / Never</td>
<td>Med / Low / None</td>
<td>High / Med / Low</td>
<td>SNR-adaptive</td>
</tr>
<tr>
<td>Demucs Usage</td>
<td>Music-detect / Category-tag / Always</td>
<td>Med / Low / High</td>
<td>High / Med / Low</td>
<td>Music-detect</td>
</tr>
<tr>
<td>LID Tagging</td>
<td>Word / Phrase / Manual</td>
<td>Low / Med / High</td>
<td>Low / Med / High</td>
<td>Phrase + heuristics</td>
</tr>
<tr>
<td>TTS Stitching</td>
<td>Crossfade / Prosody / E2E CS</td>
<td>Low / High / VHigh</td>
<td>Low / High / VHigh</td>
<td>Crossfade short-term</td>
</tr>
<tr>
<td>TTS Model</td>
<td>XTTS / MMS / F5-TTS / Drop Substack</td>
<td>Low / Low / High / None</td>
<td>Med / Med / High / N/A</td>
<td>XTTS + MMS; pursue F5-TTS</td>
</tr>
<tr>
<td><strong>Substack Pipeline</strong></td>
<td><strong>Keep / Drop / Defer</strong></td>
<td><strong>High / None / Low</strong></td>
<td><strong>Med / None / None</strong></td>
<td><strong>Defer until TTS solved</strong></td>
</tr>
<tr>
<td>Pipeline Orchestration</td>
<td>DB State / DVC / Task Queue</td>
<td>Low / Med / High</td>
<td>-</td>
<td>DB State + DVC</td>
</tr>
<tr>
<td>Validation</td>
<td>ASR check / Confidence / Human</td>
<td>Med / Low / High</td>
<td>High / Med / VHigh</td>
<td>All three layered</td>
</tr>
<tr>
<td>Dependency Management</td>
<td>Separate envs / Docker / Pin</td>
<td>Med / High / Low</td>
<td>-</td>
<td>Docker for isolation</td>
</tr>
<tr>
<td>Data Licensing</td>
<td>Research-only / Fair use / Permission</td>
<td>Low / Low / High</td>
<td>-</td>
<td>Research-only + docs</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="8-next-steps">8. Next Steps </h2>
<ol>
<li><strong>Team Review</strong>: Discuss this document and mark decisions in the matrix.</li>
<li><strong>Priority Ordering</strong>: Decide which problems to solve in v1 vs. defer to v2.</li>
<li><strong>Critical Decision: Substack Pipeline</strong>: Decide whether to keep, drop, or defer the Text-First (Substack → TTS) pipeline based on TTS model availability.</li>
<li><strong>F5-TTS Outreach</strong>: If pursuing TTS, email F5-TTS authors for model access in parallel with other work.</li>
<li><strong>Spike/POC</strong>: For uncertain areas (MFA+CS, TTS stitching quality), run small experiments before full implementation.</li>
<li><strong>Schema Updates</strong>: Add any new fields identified (e.g., <code>alignment_confidence</code>, <code>snr_db</code>, <code>is_cs</code>, <code>content_warning</code>) to <code>02_schema_v2.sql</code>.</li>
<li><strong>Dependency Audit</strong>: Test dependency compatibility for MFA + Whisper + XTTS stack. Document working versions or containerization strategy.</li>
<li><strong>Implementation</strong>: Proceed with <code>src/preprocessing/</code> module creation per the agreed plan.</li>
</ol>

      </div>
      
      
    
    
    
    
    
    
  
    </body></html>